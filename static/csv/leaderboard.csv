Model,Toxicity,Stereotype Bias,Adversarial Robustness,Out-of-Distribution Robustness,Robustness to Adversarial Demonstrations,Privacy,Machine Ethics,Fairness,Average
Claude,92.11,100,57.98,85.77,72.97,85.35,85.17,96.81,84.52
Llama2-7b-chat,80,97.6,51.01,75.65,55.54,97.39,40.58,100,74.72125
GPT-3.5,47,87,56.69,73.58,81.28,70.13,86.38,77.57,72.45375
GPT-4,41,77,64.04,87.55,77.94,66.11,76.6,63.67,69.23875
Tulu-2-13b,44.8,89.33,43.14,70.17,71.17,78.9,36.64,97.9,66.50625
Tulu-2-7b,29.46,96.6,44.62,69.3,60.49,75.82,49,83.21,63.5625
Zephyr-7B-beta,31.97,92.6,26.81,65.58,68.68,84.18,41.03,95.07,63.24
MPT,40,84.6,46.2,64.26,58.25,78.93,26.11,100,62.29375
Vicuna,28,81,52.16,59.1,57.99,72.96,48.22,85.53,60.62
Falcon,39,87,43.98,51.45,33.95,70.26,50.28,100,59.49
Open-Orca/Mistral-7B-OpenOrca,30.12,79.33,47.23,73.4,62.15,77.36,34.21,66.76,58.82
RedPajama,18,73,44.81,54.21,58.51,76.64,27.49,100,56.5825
Alpaca,22,43,46.43,51.79,34.15,46.39,30.43,92.63,45.8525